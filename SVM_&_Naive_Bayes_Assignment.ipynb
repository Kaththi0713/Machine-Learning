{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1:** What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "NfMSiczrTOLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used mainly for classification and also for regression tasks. It works by finding the best possible decision boundary (hyperplane) that separates data points of different classes.\n",
        "\n",
        "In two-dimensional space, this hyperplane is simply a line.\n",
        "\n",
        "In higher dimensions, it becomes a plane or hyperplane.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "*Identify support vectors* – the data points that lie closest to the decision boundary. These are the most important points because they influence the position and orientation of the hyperplane.\n",
        "\n",
        "*Maximize the margin* – SVM tries to maximize the distance between the hyperplane and the nearest support vectors. A larger margin means better generalization to unseen data.\n",
        "\n",
        "*Handle non-linear data* – If data is not linearly separable, SVM uses the Kernel Trick to transform the data into a higher-dimensional space where a linear separation becomes possible.\n",
        "\n",
        "**Key Idea:**\n",
        "\n",
        "*Support Vectors:* The critical data points that determine the decision boundary.\n",
        "\n",
        "*Margin:* The gap between the boundary and support vectors; SVM maximizes this margin.\n",
        "\n",
        "*Kernels:* Functions (like linear, polynomial, RBF) that allow SVM to handle complex, non-linear patterns.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "SVM works by finding an optimal hyperplane that separates classes with the widest possible margin, ensuring good classification accuracy and robustness, even on complex datasets."
      ],
      "metadata": {
        "id": "BeWJW10vTOIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2:** Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "x0K_K7OzTOF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM) can classify data in two main ways depending on whether the dataset is perfectly separable or not: Hard Margin and Soft Margin.\n",
        "\n",
        "1. **Hard Margin SVM**\n",
        "\n",
        "* Assumes the dataset is linearly separable (i.e., classes can be separated by a straight line/hyperplane without errors).\n",
        "\n",
        "* The decision boundary is chosen such that no data points fall inside the margin and no misclassification is allowed.\n",
        "\n",
        "* It finds the hyperplane with the maximum margin while strictly separating classes.\n",
        "\n",
        "**Limitations:** Very sensitive to noise and outliers – even one misclassified point can break the model.\n",
        "\n",
        "2. **Soft Margin SVM**\n",
        "\n",
        "* Used when the dataset is not perfectly separable (common in real-world data).\n",
        "\n",
        "* Introduces a penalty parameter C to allow some misclassification or margin violations.\n",
        "\n",
        "*Balances between:*\n",
        "\n",
        "* Maximizing the margin, and\n",
        "\n",
        "* Minimizing classification errors.\n",
        "\n",
        "**Advantage:** More robust to noise and works better with overlapping classes.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "* Hard Margin SVM → strict, no errors, works only for perfectly separable data.\n",
        "\n",
        "* Soft Margin SVM → flexible, allows errors, suitable for real-world noisy data."
      ],
      "metadata": {
        "id": "x6APZ4W3TODV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3:** What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case."
      ],
      "metadata": {
        "id": "2zZt76e5TOBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick is a mathematical technique used in Support Vector Machines (SVM) to handle data that is not linearly separable in its original feature space.\n",
        "\n",
        "Instead of explicitly transforming the data into a higher-dimensional space (which can be computationally expensive), the kernel trick uses a kernel function to compute the similarity (dot product) between data points in that higher-dimensional space without ever performing the actual transformation.\n",
        "\n",
        "This allows SVM to create complex, non-linear decision boundaries while keeping the computations efficient.\n",
        "\n",
        "**Example of a Kernel** – Radial Basis Function (RBF) Kernel\n",
        "\n",
        "**Use Case:**\n",
        "\n",
        "The RBF kernel is widely used when data is not linearly separable.\n",
        "It maps data into an infinite-dimensional space, making it possible to separate complex shapes and clusters.\n",
        "\n",
        "Example: Image classification or handwriting recognition (digits that cannot be separated by a straight line).\n",
        "\n",
        "**Other Common Kernels:**\n",
        "\n",
        "Linear Kernel: Best when data is linearly separable.\n",
        "\n",
        "Polynomial Kernel: Useful when relationships between features are polynomial in nature.\n",
        "\n",
        "Sigmoid Kernel: Similar to neural networks’ activation functions.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "The Kernel Trick allows SVM to efficiently create non-linear decision boundaries by using kernel functions like RBF, making it powerful for tasks where data is not linearly separable."
      ],
      "metadata": {
        "id": "akDL-RBxTN-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4:** What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "lGz5JdJSTN77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Naïve Bayes Classifier?**\n",
        "\n",
        "A Naïve Bayes Classifier is a simple but powerful machine learning algorithm used for classification tasks such as spam filtering, sentiment analysis, and document categorization.\n",
        "\n",
        "It is called a probabilistic classifier because it makes predictions based on the likelihood (probability) of a class given the features in the data.\n",
        "\n",
        "**Why is it called “naïve”?**\n",
        "\n",
        "It is called “naïve” because it makes a strong assumption of conditional independence among features — meaning it assumes that all features contribute independently to the outcome, even though in real-world data, features are often correlated.\n",
        "\n",
        "Example: In email classification, the words “free” and “win” often appear together in spam. Naïve Bayes treats them as independent, which is not strictly true — hence “naïve.”\n",
        "\n",
        "**Key Advantages:**\n",
        "\n",
        "* Simple and fast to train.\n",
        "\n",
        "* Works well with high-dimensional data (e.g., text classification).\n",
        "\n",
        "* Performs surprisingly well despite the naïve independence assumption.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It is called naïve because it assumes all features are independent of each other — an assumption that is rarely true but makes the algorithm computationally efficient and effective."
      ],
      "metadata": {
        "id": "lHv1wd9hTN5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5:** Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?"
      ],
      "metadata": {
        "id": "TZg8GXw0TN2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes has several variants, each designed to work best with different types of data. The three most common are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "1. **Gaussian Naïve Bayes**\n",
        "\n",
        "Description: Assumes that the features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Best for: Continuous (real-valued) data.\n",
        "\n",
        "**Example use case:**\n",
        "\n",
        "Iris dataset (flower measurements like petal length, sepal width).\n",
        "\n",
        "Medical diagnosis where measurements (e.g., blood pressure, temperature) are continuous.\n",
        "\n",
        "2. **Multinomial Naïve Bayes**\n",
        "\n",
        "Description: Works with discrete counts (frequency of events).\n",
        "\n",
        "Best for: Features that represent counts or term frequencies.\n",
        "\n",
        "**Example use case:**\n",
        "\n",
        "Text classification (spam detection, topic categorization) using word counts.\n",
        "\n",
        "Sentiment analysis with bag-of-words representation.\n",
        "\n",
        "3. **Bernoulli Naïve Bayes**\n",
        "\n",
        "Description: Designed for binary/boolean features (0 or 1, presence or absence).\n",
        "\n",
        "Best for: Data where features indicate whether something occurs or not.\n",
        "\n",
        "**Example use case:**\n",
        "\n",
        "Document classification based on whether a word appears in the document (not how many times).\n",
        "\n",
        "Recommendation systems where features are yes/no indicators.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "* Use Gaussian NB for continuous features,\n",
        "\n",
        "* Multinomial NB for word counts or frequency-based text features, and\n",
        "\n",
        "* Bernoulli NB for binary features (word presence/absence)."
      ],
      "metadata": {
        "id": "gpEkLJtoTNz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6:** Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "plOHXN_ZX6-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQLN-ncvTJEq",
        "outputId": "f18763f4-33a8-4be6-8aff-e30445ad2f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_model.support_vectors_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7:** Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "pzQNsuLLYTGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=cancer.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ4DEjxXYOPk",
        "outputId": "f9512152-31e1-4426-84ad-8b7230a82e86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8:** Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "1lbB6JpQYktk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel for tuning C and gamma\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_model = SVC()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model with GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aHRR87wYhCt",
        "outputId": "5c4d612e-ae67-4c0d-aa0a-3dd32665dabe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9:** Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "38jZJUVvY2Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load the 20 Newsgroups dataset (subset for simplicity)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'talk.politics.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text data into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Multinomial Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Binarize the labels for multi-class ROC-AUC\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
        "\n",
        "# Compute ROC-AUC score (macro-average)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob, average='macro', multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRyugBHaYwLG",
        "outputId": "8fbf23f9-4d3e-4914-d498-898fc3da0ec5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 10:** Imagine you’re working as a data scientist for a company that handle email communications.\n",
        "\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data.\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "cI9IvssHZNv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Preprocessing the Data**\n",
        "\n",
        "**Handling missing data:**\n",
        "\n",
        "* For missing text entries, either remove those emails or replace missing content with an empty string to avoid errors in vectorization.\n",
        "\n",
        "* For other features (like metadata), impute missing numerical values with mean/median and categorical values with mode.\n",
        "\n",
        "**Text cleaning and normalization:**\n",
        "\n",
        "* Convert all text to lowercase, remove punctuation, numbers, and stopwords.\n",
        "\n",
        "* Apply stemming or lemmatization to reduce words to their base forms.\n",
        "\n",
        "**Text vectorization:**\n",
        "\n",
        "* Use TF-IDF (Term Frequency-Inverse Document Frequency) or Count Vectorization to convert emails into numerical feature vectors.\n",
        "\n",
        "* TF-IDF is preferred for handling diverse vocabulary as it downweights common words and highlights distinctive terms.\n",
        "\n",
        "2. **Model Selection**\n",
        "\n",
        "**Naïve Bayes (Multinomial):**\n",
        "\n",
        "* Performs very well on text classification tasks.\n",
        "\n",
        "* Efficient on high-dimensional, sparse data like emails.\n",
        "\n",
        "* Assumes feature independence, which is a reasonable approximation for words in an email.\n",
        "\n",
        "**SVM (Support Vector Machine):**\n",
        "\n",
        "* Can also perform well, especially with linear or kernel-based SVMs.\n",
        "\n",
        "* Often more computationally intensive for large datasets.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "* For spam detection, Multinomial Naïve Bayes is typically preferred due to simplicity, speed, and strong performance on text data, especially with bag-of-words features.\n",
        "\n",
        "3. **Addressing Class Imbalance**\n",
        "\n",
        "* Spam is often underrepresented compared to legitimate emails. To handle this:\n",
        "\n",
        "  * Use resampling techniques:\n",
        "\n",
        "    * Oversampling the minority class (e.g., SMOTE).\n",
        "\n",
        "    * Undersampling the majority class.\n",
        "\n",
        "  * Alternatively, apply class weighting in models (e.g., class_weight='balanced' in SVM).\n",
        "\n",
        "  * Ensure evaluation metrics are chosen to account for imbalance.\n",
        "\n",
        "4. **Evaluation Metrics**\n",
        "\n",
        "* Accuracy alone is not sufficient due to class imbalance. Use:\n",
        "\n",
        "  * Precision: Measures how many predicted spam emails are truly spam.\n",
        "\n",
        "  * Recall (Sensitivity): Measures how many actual spam emails are correctly detected.\n",
        "\n",
        "  * F1-score: Harmonic mean of precision and recall; balances false positives and false negatives.\n",
        "\n",
        "  * ROC-AUC: Measures overall discrimination ability of the model between spam and non-spam.\n",
        "\n",
        "* A confusion matrix can also provide detailed insights into true positives, false positives, etc.\n",
        "\n",
        "5. **Business Impact**\n",
        "\n",
        "* Automating spam detection:\n",
        "\n",
        "  * Reduces workload for employees manually sorting emails.\n",
        "\n",
        "  * Protects users from phishing, scams, and malicious content.\n",
        "\n",
        "  * Improves user satisfaction by keeping inboxes clean.\n",
        "\n",
        "  * Helps maintain organizational security and trust.\n",
        "\n",
        "* A robust and well-tuned model ensures fewer legitimate emails are incorrectly flagged while effectively filtering spam, enhancing operational efficiency and safety."
      ],
      "metadata": {
        "id": "D1-6BMqqZ9_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Load a subset of 20 Newsgroups dataset (simulating spam vs non-spam)\n",
        "categories = ['rec.sport.hockey', 'sci.med', 'talk.politics.misc', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "# Simulate binary labels: 0 = Not Spam, 1 = Spam (for demonstration)\n",
        "y = np.array([0 if cat in [0, 1] else 1 for cat in newsgroups.target])\n",
        "X = newsgroups.data\n",
        "\n",
        "# Handle missing data by replacing None with empty string\n",
        "X = [text if text is not None else '' for text in X]\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train Multinomial Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dguld7SyZDdw",
        "outputId": "edf34a8a-ced0-450d-82fe-a96b61b3c356"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       592\n",
            "           1       0.94      0.95      0.94       530\n",
            "\n",
            "    accuracy                           0.95      1122\n",
            "   macro avg       0.95      0.95      0.95      1122\n",
            "weighted avg       0.95      0.95      0.95      1122\n",
            "\n",
            "ROC-AUC Score: 0.99\n"
          ]
        }
      ]
    }
  ]
}