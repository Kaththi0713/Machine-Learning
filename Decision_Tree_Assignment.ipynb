{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is a Decision Tree, and how does it work in the context of classification?**"
      ],
      "metadata": {
        "id": "DrETw9CVsksF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it works by breaking down a dataset into smaller subsets based on feature values, following a tree-like structure.\n",
        "\n",
        "**Structure:**\n",
        "\n",
        "* Each internal node represents a decision based on a feature (e.g., \"Is petal length > 2.5 cm?\").\n",
        "\n",
        "* Each branch represents the outcome of that decision.\n",
        "\n",
        "* Each leaf node represents a class label (final prediction).\n",
        "\n",
        "**Working:**\n",
        "\n",
        "The algorithm starts at the root node and recursively splits the dataset based on the feature that provides the most information (measured by impurity measures like Gini or Entropy). This continues until the tree reaches pure subsets (all samples in a node belong to one class) or a stopping condition (like maximum depth).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "In the Iris dataset, a decision tree might first split on petal length to separate Setosa from the other species, then further split on petal width to distinguish Versicolor and Virginica.\n",
        "\n",
        "**Key point:** A Decision Tree classifies data by asking a sequence of \"yes/no\" questions about features until it reaches a conclusion."
      ],
      "metadata": {
        "id": "mGBtquUoski8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "JBOij-Z4skgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity and Entropy in Decision Trees\n",
        "\n",
        "When building a Decision Tree, the algorithm must decide which feature and threshold to split on at each step. To do this, it uses impurity measures to evaluate how “pure” a node is (i.e., how mixed the classes are). Two commonly used impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "**Gini Impurity**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Gini impurity measures the probability that a randomly chosen sample would be misclassified if it were randomly labeled according to the distribution of labels in the node.\n",
        "\n",
        "**Range:**\n",
        "\n",
        "0 → node is pure (only one class).\n",
        "\n",
        "Maximum (0.5 for binary classification) → highly impure, classes evenly mixed.\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Entropy measures the amount of uncertainty or disorder in the dataset. It comes from information theory.\n",
        "\n",
        "**Range:**\n",
        "\n",
        "0 → node is pure (all samples belong to one class).\n",
        "\n",
        "Maximum (1 for binary classification when classes are 50/50) → maximum disorder.\n",
        "\n",
        "**Impact on Splits**\n",
        "\n",
        "* The Decision Tree algorithm tries to reduce impurity at each split.\n",
        "\n",
        "* It calculates the Information Gain (reduction in impurity) for each possible split and chooses the feature/threshold with the highest gain.\n",
        "\n",
        "* Using Gini or Entropy usually leads to similar trees, but:\n",
        "\n",
        "* Gini is faster computationally and tends to create slightly purer nodes.\n",
        "\n",
        "* Entropy is more information-theoretic and considers uncertainty in greater detail.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "* Both Gini and Entropy measure how mixed the classes are in a node.\n",
        "\n",
        "* Decision Trees split data based on the feature that produces the largest reduction in impurity, ensuring more homogeneous child nodes and better classification performance."
      ],
      "metadata": {
        "id": "1voWDV6Mskds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question** 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
      ],
      "metadata": {
        "id": "El0f5Qm_skbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning vs Post-Pruning in Decision Trees\n",
        "\n",
        "Decision Trees have a tendency to overfit the training data if allowed to grow without restrictions. To address this, pruning techniques are used to control tree growth and improve generalization.\n",
        "\n",
        "**Pre-Pruning (Early Stopping)**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Pre-pruning stops the growth of the tree early, before it becomes overly complex. The algorithm imposes constraints such as:\n",
        "\n",
        "Maximum tree depth (max_depth)\n",
        "\n",
        "Minimum number of samples required to split a node (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Maximum number of leaf nodes\n",
        "\n",
        "**Advantage:**\n",
        "\n",
        "It reduces computation time and prevents the tree from becoming too deep, thus lowering the risk of overfitting.\n",
        "\n",
        "**Post-Pruning (Reduced Error Pruning)**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Post-pruning allows the tree to grow to its full size first, then removes branches that do not contribute significantly to accuracy. This is done by evaluating performance on a validation set and pruning branches that don’t improve generalization.\n",
        "\n",
        "**Advantage:**\n",
        "\n",
        "It produces a simpler and more generalizable model, improving accuracy on unseen data by eliminating unnecessary complexity.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "Pre-Pruning: Stops tree growth early → faster training.\n",
        "\n",
        "Post-Pruning: Trims the fully grown tree → better generalization."
      ],
      "metadata": {
        "id": "5jyqY6i4skYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question** 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ],
      "metadata": {
        "id": "95uVuXjrskWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information Gain in Decision Trees**\n",
        "\n",
        "When building a Decision Tree, the algorithm must decide which feature to split on at each step. This decision is guided by Information Gain (IG).\n",
        "\n",
        "**Definition**\n",
        "\n",
        "Information Gain (IG) measures the reduction in impurity (uncertainty) after a dataset is split on a feature.\n",
        "\n",
        "It is based on Entropy, which quantifies the amount of disorder or randomness in a dataset.\n",
        "\n",
        "**Why it is Important**\n",
        "\n",
        "Information Gain helps the Decision Tree choose the best split at each node.\n",
        "\n",
        "A feature with high IG means splitting on it makes the resulting subsets more “pure” (closer to containing only one class).\n",
        "\n",
        "This leads to shorter, more efficient trees and better classification performance.\n",
        "\n",
        "Without IG (or similar measures like Gini), the algorithm would not know which feature provides the most useful separation of classes.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we are classifying whether a student passes or fails based on “Study Hours.”\n",
        "\n",
        "* Before splitting: Dataset has 50% Pass, 50% Fail → high entropy.\n",
        "\n",
        "* After splitting:\n",
        "\n",
        "  * Group 1 (Study Hours > 5): 90% Pass, 10% Fail → lower entropy.\n",
        "\n",
        "  * Group 2 (Study Hours ≤ 5): 20% Pass, 80% Fail → lower entropy.\n",
        "\n",
        "The split reduces uncertainty, so “Study Hours” has high Information Gain.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "Information Gain tells us how much a feature improves classification by reducing uncertainty. The Decision Tree always chooses the feature with the highest IG at each step, making it a key factor in building accurate and efficient models."
      ],
      "metadata": {
        "id": "sC1bSqwJskT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question** 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "8NcuS_cmskRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision Trees are widely used in different industries because they are easy to interpret, handle both numerical and categorical data, and require little preprocessing. Some common applications include:\n",
        "\n",
        "1. **Healthcare**\n",
        "\n",
        "Predicting whether a patient has a disease based on symptoms, lab results, and medical history.\n",
        "\n",
        "Assisting doctors with diagnostic decision-making.\n",
        "\n",
        "2. **Finance & Banking**\n",
        "\n",
        "Credit scoring (deciding whether to approve a loan).\n",
        "\n",
        "Fraud detection in transactions.\n",
        "\n",
        "3. **Marketing & Customer Analytics**\n",
        "\n",
        "Predicting customer churn (whether a customer will leave).\n",
        "\n",
        "Recommending products based on past purchases.\n",
        "\n",
        "Segmenting customers for targeted marketing campaigns.\n",
        "\n",
        "4. **Operations & Manufacturing**\n",
        "\n",
        "Predictive maintenance of machines.\n",
        "\n",
        "Quality control (classifying defective vs. non-defective products).\n",
        "\n",
        "5. **Education**\n",
        "\n",
        "Predicting student performance based on attendance, assignments, and test scores.\n",
        "\n",
        "**Advantages of Decision Trees**\n",
        "\n",
        "* Easy to interpret and visualize → Trees resemble human decision-making.\n",
        "\n",
        "* Handles both numerical and categorical data without much preprocessing.\n",
        "\n",
        "* Non-parametric → No assumption about data distribution.\n",
        "\n",
        "* Can capture nonlinear relationships between features and target.\n",
        "\n",
        "**Limitations of Decision Trees**\n",
        "\n",
        "* Prone to overfitting if grown too deep.\n",
        "\n",
        "* Unstable → Small changes in data can lead to very different trees.\n",
        "\n",
        "* Biased towards features with more categories (especially in categorical data).\n",
        "\n",
        "* Less accurate alone compared to ensemble methods like Random Forest or Gradient Boosted Trees.\n",
        "\n",
        "**In summary:**\n",
        "Decision Trees are powerful and interpretable tools used in healthcare, finance, marketing, and many other fields. Their main strengths are simplicity and interpretability, while their weaknesses lie in overfitting and instability."
      ],
      "metadata": {
        "id": "cbyXwyJZxcXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Info:**\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "# **Question 6: Write a Python program to:**\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances."
      ],
      "metadata": {
        "id": "QZmFxJ39ynIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Hz5DcKseB5",
        "outputId": "4c7fdda6-880d-47e0-8587-c4c3246218b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Feature Importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:**\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "NwzRtZnPzWiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "full_acc = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "depth3_acc = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Print results\n",
        "print(\"Full Tree Accuracy:\", full_acc)\n",
        "print(\"Max Depth=3 Accuracy:\", depth3_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwxXyUpvy7WI",
        "outputId": "59a4f86b-08f9-418b-febf-a13067bddb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to:**\n",
        "\n",
        "● Load the California Housing dataset from sklearn\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "V6UKMEXh0egy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Feature Importances\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcvAkbNqzxQ8",
        "outputId": "5e057d17-e8ec-437d-ca4f-6418c3a61877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to:**\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "T6z-gWmD0_cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,              # 5-fold cross-validation\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test data\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4_SfsM0ste",
        "outputId": "1d914c4b-7b76-4f1a-ec3e-e2fa1f4a49b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: Imagine you’re working as a data scientist for a  Healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting"
      ],
      "metadata": {
        "id": "pWqtGRLe1hmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Handling Missing Values**\n",
        "\n",
        "Missing data can bias predictions or reduce model accuracy. Steps include:\n",
        "\n",
        "*Identify missing values:* Check each column for missing data using methods like .isnull().sum().\n",
        "\n",
        "**Decide on imputation strategy:**\n",
        "\n",
        "*Numerical features:* Impute missing values with mean, median, or a model-based approach (e.g., KNN imputer) depending on distribution.\n",
        "\n",
        "*Categorical features:* Impute with the mode (most frequent value) or create a special category “Unknown”.\n",
        "\n",
        "*Optional:* If a column has >50% missing values, consider dropping it as it may not provide useful information."
      ],
      "metadata": {
        "id": "PAdC02sO2Mls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Encoding Categorical Features**\n",
        "\n",
        "Decision Trees in most libraries handle numerical data, so categorical features must be encoded:\n",
        "\n",
        "*Label Encoding:* Assign numeric values to each category (useful if categories are ordinal).\n",
        "\n",
        "*One-Hot Encoding:* Create binary columns for each category (preferred for nominal variables to avoid implying order).\n",
        "\n",
        "*Avoid high cardinality:* For categorical features with many unique values, consider grouping rare categories into “Other” to reduce dimensionality."
      ],
      "metadata": {
        "id": "w6Tn_IoA2Yo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Train a Decision Tree Model**\n",
        "\n",
        "Decision Trees are robust to non-linear relationships and don’t require feature scaling.\n",
        "\n",
        "*Split data:* Divide into training and testing sets (e.g., 80/20 split).\n",
        "\n",
        "*Initialize model:* Use DecisionTreeClassifier() from scikit-learn.\n",
        "\n",
        "*Fit the model:* Train on the preprocessed training dataset."
      ],
      "metadata": {
        "id": "_uMni_gy2slk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "89-PkF4v2_ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Hyperparameter Tuning**\n",
        "\n",
        "To avoid overfitting and improve performance:\n",
        "\n",
        "**Key parameters to tune:**\n",
        "\n",
        "*max_depth:* Maximum depth of the tree.\n",
        "\n",
        "*min_samples_split:* Minimum samples required to split a node.\n",
        "\n",
        "*min_samples_leaf:* Minimum samples required at a leaf node.\n",
        "\n",
        "*criterion:* “gini” or “entropy”.\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV for systematic search:"
      ],
      "metadata": {
        "id": "VSN5uVer2sYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                           param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "cmhtCxb_3Sbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Evaluate Performance**\n",
        "\n",
        "For a healthcare prediction model, evaluation metrics should include:\n",
        "\n",
        "*Accuracy:* General correctness, but not enough for imbalanced data.\n",
        "\n",
        "*Precision & Recall:* Critical if false negatives (missing disease) are costly.\n",
        "\n",
        "*F1-Score:* Balance between precision and recall.\n",
        "\n",
        "*ROC-AUC:* Measures discrimination ability across thresholds."
      ],
      "metadata": {
        "id": "S6EALQs02r9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))"
      ],
      "metadata": {
        "id": "Lt-9giZm1G3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Optionally, visualize feature importance to understand key predictors  of disease."
      ],
      "metadata": {
        "id": "fmSf4CSK3pLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Business Value**\n",
        "\n",
        "*Early disease detection:* Helps doctors identify high-risk patients quickly.\n",
        "\n",
        "*Resource optimization:* Prioritizes patients for further testing or treatment, reducing unnecessary costs.\n",
        "\n",
        "*Personalized healthcare:* Enables tailored treatment plans based on patient risk profiles.\n",
        "\n",
        "*Data-driven decision making:* Supports hospital management and policy decisions using predictive insights."
      ],
      "metadata": {
        "id": "gNuKekxC30zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "This workflow ensures clean, well-prepared data, optimizes model performance, and provides actionable insights in a healthcare context. Decision Trees offer interpretability, making it easier for medical professionals to trust predictions."
      ],
      "metadata": {
        "id": "AU2UJ-rL4AN3"
      }
    }
  ]
}