{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Logistic Regression, and how does it differ from Linear Regression?**"
      ],
      "metadata": {
        "id": "U1nAZHkZnxD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "* Logistic Regression is a statistical and machine learning algorithm used for classification problems, especially when the output (dependent variable) is categorical.\n",
        "\n",
        "* It predicts the probability that an observation belongs to a certain class (e.g., Yes/No, Spam/Not Spam).\n",
        "\n",
        "* The output is passed through a sigmoid (logistic) function, which maps values to a range between 0 and 1.\n",
        "\n",
        "* A threshold (commonly 0.5) is then applied to decide the final class.\n",
        "\n",
        "**Example:** Predicting whether a customer will buy a product (Yes=1, No=0).\n",
        "\n",
        "**Linear Regression**\n",
        "\n",
        "* Linear Regression is used for predicting continuous outcomes.\n",
        "\n",
        "* It assumes a linear relationship between the independent variables (X) and the dependent variable (Y).\n",
        "\n",
        "* The output is a real number that can range from ‚àí‚àû to +‚àû.\n",
        "\n",
        "**Example:** Predicting house prices based on area, location, and number of rooms.\n",
        "\n",
        "**In short:**\n",
        "\n",
        "Linear Regression ‚Üí Continuous prediction.\n",
        "\n",
        "Logistic Regression ‚Üí Classification (probability-based)."
      ],
      "metadata": {
        "id": "QdEPnG1anxAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the role of the Sigmoid function in Logistic Regression.**"
      ],
      "metadata": {
        "id": "03Du0Tdxnw9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression starts with a linear combination of inputs:\n",
        "\n",
        "ùëß = ùõΩ0 + ùõΩ1ùë•1 + ùõΩ2ùë•2 + ‚ãØ + ùõΩùëõùë•ùëõ\n",
        "\n",
        "But if we use this directly, it can output any value between ‚àí‚àû and +‚àû.\n",
        "That‚Äôs not useful when we need probabilities (must be between 0 and 1).\n",
        "\n",
        "The sigmoid function (also called logistic function) fixes this:\n",
        "\n",
        "ùúé(ùëß) = 1/1+e‚àíz\n",
        "\n",
        "**Role of the Sigmoid function:**\n",
        "\n",
        "Maps outputs to probabilities (0‚Äì1) ‚Üí ensures predictions are interpretable as probabilities.\n",
        "\n",
        "Decision boundary: If probability ‚â• 0.5 ‚Üí Class 1, else Class 0.\n",
        "\n",
        "Smooth & differentiable ‚Üí makes optimization with gradient descent possible.\n",
        "\n",
        "Probabilistic interpretation ‚Üí output tells how likely the input belongs to the positive class."
      ],
      "metadata": {
        "id": "6jE-EqnFnw7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**"
      ],
      "metadata": {
        "id": "ZVikDWcynw4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique in Logistic Regression (and other ML models) that adds a penalty term to the loss function to prevent the model from overfitting.\n",
        "\n",
        "The idea: when coefficients become too large, the model fits the training data extremely well but performs poorly on new data (poor generalization).\n",
        "\n",
        "Regularization keeps coefficients smaller and more stable, so the model balances fit and simplicity.\n",
        "\n",
        "**Types of Regularization:**\n",
        "\n",
        "*L1 Regularization (Lasso):*\n",
        "\n",
        "Adds ùúÜ‚àë‚à£ùõΩùëó‚à£ to the cost function.\n",
        "\n",
        "Forces some coefficients to become exactly zero ‚Üí performs feature selection.\n",
        "\n",
        "*L2 Regularization (Ridge):*\n",
        "\n",
        "Adds ùúÜ‚àëùõΩùëó2 to the cost function.\n",
        "\n",
        "Shrinks coefficients towards zero but never makes them exactly zero.\n",
        "\n",
        "Helps when features are correlated (multicollinearity).\n",
        "\n",
        "*Elastic Net:*\n",
        "\n",
        "Combination of L1 and L2 penalties.\n",
        "\n",
        "**Why Regularization is needed:**\n",
        "\n",
        "* Prevents overfitting (model memorizing noise in training data).\n",
        "\n",
        "* Improves generalization on unseen data.\n",
        "\n",
        "* Helps with multicollinearity by shrinking correlated feature weights.\n",
        "\n",
        "* Encourages sparser models (L1) or more stable models (L2)."
      ],
      "metadata": {
        "id": "cbu_81uDnwyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are some common evaluation metrics for classification models, and why are they important?**"
      ],
      "metadata": {
        "id": "zT4BO9_unwus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating classification models (like Logistic Regression), accuracy alone is not enough, especially when the dataset is imbalanced (e.g., fraud detection, medical diagnosis).\n",
        "That‚Äôs why we use multiple evaluation metrics:\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "* Measures overall % of correct predictions.\n",
        "\n",
        "* Misleading if classes are imbalanced (e.g., predicting everyone as ‚Äúnegative‚Äù in a 95%-5% dataset gives 95% accuracy but is useless).\n",
        "\n",
        "2. Precision (Positive Predictive Value)\n",
        "\n",
        "* Out of all predicted positives, how many are actually positive?\n",
        "\n",
        "* Important when the cost of false positives is high.\n",
        "\n",
        "**Example:** Predicting spam emails ‚Üí better to avoid labeling normal emails as spam.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "* Out of all actual positives, how many did we catch?\n",
        "\n",
        "* Important when the cost of false negatives is high.\n",
        "\n",
        "**Example:** In cancer detection, missing a positive case is very costly.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "* Harmonic mean of Precision and Recall.\n",
        "\n",
        "* Useful when we want a balance between avoiding false positives & false negatives.\n",
        "\n",
        "5. ROC-AUC (Receiver Operating Characteristic ‚Äì Area Under Curve)\n",
        "\n",
        "* Plots True Positive Rate (Recall) vs False Positive Rate (FP / (FP + TN)) at different thresholds.\n",
        "\n",
        "* AUC = probability that the model ranks a random positive higher than a random negative.\n",
        "\n",
        "* Closer to 1.0 ‚Üí better.\n",
        "\n",
        "6. PR-AUC (Precision-Recall AUC)\n",
        "\n",
        "* Plots Precision vs Recall across thresholds.\n",
        "\n",
        "* More informative than ROC-AUC when classes are highly imbalanced.\n",
        "\n",
        "**Why these metrics are important?**\n",
        "\n",
        "Accuracy: Good only when classes are balanced.\n",
        "\n",
        "Precision & Recall: Help us understand trade-offs between catching positives and avoiding false alarms.\n",
        "\n",
        "F1: Balances Precision & Recall in one number.\n",
        "\n",
        "ROC-AUC & PR-AUC: Evaluate model performance across thresholds instead of at just one cut-off."
      ],
      "metadata": {
        "id": "3pc-yyTKnwr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.**"
      ],
      "metadata": {
        "id": "Ypmpei-SnwpQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vin_nqxntD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0759856-1c9a-4ead-ed50-be7c3782e0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (178, 14)\n",
            "Class distribution:\n",
            " target\n",
            "1    71\n",
            "0    59\n",
            "2    48\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.9444444444444444\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "df = pd.DataFrame(X, columns=wine.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Class distribution:\\n\", df['target'].value_counts())\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=['target']),\n",
        "    df['target'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['target']\n",
        ")\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000, multi_class='ovr')  # one-vs-rest for multiclass\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predictions & Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.**"
      ],
      "metadata": {
        "id": "tnGuWAXTYhFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "# Step 2: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=5000, C=1.0)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predictions & Accuracy\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Model Coefficients\n",
        "coef_df = pd.DataFrame(model.coef_, columns=feature_names)\n",
        "coef_df['class'] = wine.target_names\n",
        "\n",
        "print(\"Test Accuracy:\", acc)\n",
        "print(\"\\nLogistic Regression Coefficients (per class):\")\n",
        "print(coef_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyKIchbfYWXX",
        "outputId": "c7b8fbb7-4a4a-4312-b549-738e564a51b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0\n",
            "\n",
            "Logistic Regression Coefficients (per class):\n",
            "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
            "0  1.210972    0.415617  0.805159          -1.359310   0.096232   \n",
            "1 -1.307149   -0.790072 -1.169551           0.697742  -0.199082   \n",
            "2  0.276977    0.483434  0.478266           0.406877   0.018923   \n",
            "\n",
            "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
            "0       0.254444    1.120447             -0.006312        -0.248447   \n",
            "1       0.087130    0.338720              0.206824         0.379657   \n",
            "2      -0.160326   -1.349051             -0.144092        -0.425197   \n",
            "\n",
            "   color_intensity       hue  od280/od315_of_diluted_wines   proline    class  \n",
            "0         0.185990  0.001280                      0.953953  1.645212  class_0  \n",
            "1        -1.869870  0.905697                     -0.204198 -1.685038  class_1  \n",
            "2         1.448417 -0.895848                     -0.662348  0.066522  class_2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.**"
      ],
      "metadata": {
        "id": "4iIZ7DyDYtNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "target_names = wine.target_names\n",
        "\n",
        "# Step 2: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with OVR\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=5000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predictions & Evaluation\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", acc)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Qj3GkgYrAN",
        "outputId": "2afe0d95-a992-4e5e-f73f-2cf80750d3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        12\n",
            "     class_1       1.00      1.00      1.00        14\n",
            "     class_2       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        36\n",
            "   macro avg       1.00      1.00      1.00        36\n",
            "weighted avg       1.00      1.00      1.00        36\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.**"
      ],
      "metadata": {
        "id": "tCnJTaTJZAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 2: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Define Logistic Regression and parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=5000, multi_class='ovr')\n",
        "\n",
        "# Step 5: GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Print best parameters and validation accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg15daLzY5xj",
        "outputId": "a7e20808-470e-405f-f4d6-89fb4642b3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.993103448275862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.**"
      ],
      "metadata": {
        "id": "QV-22v4nZQEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 2: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3a: Train Logistic Regression without scaling\n",
        "model_no_scale = LogisticRegression(solver='liblinear', multi_class='ovr', max_iter=5000)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# Step 3b: Train Logistic Regression with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(solver='liblinear', multi_class='ovr', max_iter=5000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 4: Print results\n",
        "print(\"Accuracy without scaling:\", acc_no_scale)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXTLcxhmZG3e",
        "outputId": "cfd7a701-8617-4fd0-d12c-e9b8baabb92d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9722222222222222\n",
            "Accuracy with scaling: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a Logistic Regression model ‚Äî including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**"
      ],
      "metadata": {
        "id": "0ThBQYRfZw2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Problem framing & metric selection**\n",
        "\n",
        "* Goal: Identify customers likely to respond (positive class = responder). Only 5% responders ‚Üí high class imbalance.\n",
        "\n",
        "* Primary metrics: Precision@k, Recall, Precision-Recall AUC (average precision), Lift/Gain, and business-metric (expected profit).\n",
        "\n",
        "* Do not rely on accuracy. Use PR-AUC / precision@topK because they reflect performance on rare class.\n",
        "\n",
        "**2. Data preparation / EDA**\n",
        "\n",
        "* Inspect class distribution, missing values, feature types, correlations, outliers.\n",
        "\n",
        "* Visualize positive vs negative distributions for key features.\n",
        "\n",
        "* Handle missing data: impute (median / KNN / iterative) based on feature type and missingness mechanism.\n",
        "\n",
        "* Outliers: Winsorize or clip if they are data errors; otherwise consider robust scaling.\n",
        "\n",
        "**3. Feature engineering**\n",
        "\n",
        "* Create interaction features (e.g., recency √ó frequency), binned continuous variables, one-hot or target-encoding for high-cardinality categoricals.\n",
        "\n",
        "* Use domain knowledge ‚Äî e.g., recency of last purchase, total spend, categorical membership.\n",
        "\n",
        "**4. Train/validation split**\n",
        "\n",
        "* Use Stratified split so the 5% class ratio is preserved.\n",
        "\n",
        "* Prefer StratifiedKFold for cross-validation to preserve imbalance in folds.\n",
        "\n",
        "**5. Balancing strategies (choose one or combine)**\n",
        "\n",
        "* Model-level (fast, safe):\n",
        "\n",
        "* Use class_weight='balanced' in LogisticRegression ‚Äî often a good baseline (no resampling)."
      ],
      "metadata": {
        "id": "l69HDkA_adol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')"
      ],
      "metadata": {
        "id": "JnrdAcwwAqVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Data-level (resampling):\n",
        "\n",
        "* SMOTE (synthetic oversampling) to increase minority class in training only:"
      ],
      "metadata": {
        "id": "3yNHbunLakR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "HCA4Vir8amgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Combine SMOTE + Tomek or SMOTEENN to oversample then clean.\n",
        "\n",
        "* Hybrid / ensemble:\n",
        "\n",
        "* BalancedBaggingClassifier, or ensemble of models trained on different balanced samples.\n",
        "\n",
        "* Important: only resample the training set. Keep validation/test sets untouched for realistic evaluation.\n",
        "\n",
        "**6. Feature scaling**\n",
        "\n",
        "* Use StandardScaler (or MinMax) when using regularized logistic regression. Fit scaler on training set and transform validation/test.\n",
        "\n",
        "**7. Hyperparameter tuning**\n",
        "\n",
        "* Tune C (inverse of regularization strength), penalty (l1,l2), and solver. Use Stratified CV.\n",
        "\n",
        "* For imbalanced data, optimize for PR-AUC or average_precision:"
      ],
      "metadata": {
        "id": "O3UaVwAiamHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(LogisticRegression(solver='liblinear'),\n",
        "                    param_grid={'C':[0.01,0.1,1,10],'penalty':['l1','l2']},\n",
        "                    scoring='average_precision', cv=5)"
      ],
      "metadata": {
        "id": "CVALtCxcasY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Consider nested CV if you want unbiased performance estimates.\n",
        "\n",
        "**8. Probability calibration & threshold tuning**\n",
        "\n",
        "* Logistic regression gives probabilities; they may need calibration:"
      ],
      "metadata": {
        "id": "USnfiIvSatf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "calibrated = CalibratedClassifierCV(base_model, cv=3)\n",
        "calibrated.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XMQdZzQuaxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Threshold selection:\n",
        "\n",
        "* Pick a probability threshold not necessarily 0.5. Choose threshold based on business metric (max expected profit, or required precision@k).\n",
        "\n",
        "* Use precision-recall curve to pick a threshold that gives acceptable precision / recall trade-off:"
      ],
      "metadata": {
        "id": "hoDntgf5a0X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "probs = model.predict_proba(X_val)[:,1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, probs)"
      ],
      "metadata": {
        "id": "Y4QqhATha2yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Evaluate precision@k (top k customers) and expected revenue per targeted customer.\n",
        "\n",
        "**9. Evaluation ‚Äî practical business metrics**\n",
        "\n",
        "* Report: Precision, Recall, F1 (for chosen threshold), PR-AUC (average precision), Lift/Gain charts, Confusion matrix, Precision@k.\n",
        "\n",
        "* For business: compute expected profit using:\n",
        "\n",
        "* revenue per positive, cost per contact, and predicted counts at threshold ‚Üí choose threshold maximizing expected value.\n",
        "\n",
        "* Monitor false positives cost (contacting uninterested customers) vs false negatives lost revenue.\n",
        "\n",
        "**10. Explainability & monitoring**\n",
        "\n",
        "* Logistic regression coefficients are interpretable ‚Äî show top drivers of response.\n",
        "\n",
        "* Deploy monitoring: population drift, model calibration, feature drift.\n",
        "\n",
        "* Retrain periodically and keep a labeled feedback loop."
      ],
      "metadata": {
        "id": "mvr8lBHXa6fe"
      }
    }
  ]
}